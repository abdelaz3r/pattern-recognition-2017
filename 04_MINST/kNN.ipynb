{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "University of Fribourg\n",
    "\n",
    "Departement of Informatics\n",
    "\n",
    "SS 2017\n",
    "\n",
    "*********************\n",
    "# Pattern Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make rows names more readable\n",
    "def new_columns_names_train(columns_size):\n",
    "    new_columns_array = np.array(['label'])\n",
    "\n",
    "    for i in range(1,size_train):\n",
    "        new_name = 'pixel'+ str(i)\n",
    "        new_columns_array = np.append(new_columns_array,[new_name])\n",
    "        \n",
    "    return new_columns_array\n",
    "\n",
    "def new_columns_names_test(columns_size):\n",
    "    new_columns_array = np.array(['pixel0'])\n",
    "    for i in range(1,size_test):\n",
    "        new_name = 'pixel'+ str(i)\n",
    "        new_columns_array = np.append(new_columns_array, [new_name])\n",
    "        \n",
    "    return new_columns_array\n",
    "\n",
    "# Read csv files\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./mnist_test.csv\")\n",
    "\n",
    "#Create the corresponding data frames\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "\n",
    "#Size of columns array must be the same between training data and test data\n",
    "size_train = len(train_df.columns.values)\n",
    "size_test = len(test_df.columns.values)\n",
    "\n",
    "\n",
    "#Compute the new data frame\n",
    "train_df.columns = new_columns_names_train(size_train)\n",
    "test_df.columns =  new_columns_names_test(size_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26998, 784) (9999, 784) (26998,)\n"
     ]
    }
   ],
   "source": [
    "#KNN is non-parametric, instance-based and used in a supervised learning setting.\n",
    "#X_train reprensents features of our taining set --> pixels\n",
    "#Y_train represents labels of our training set --> number in the image\n",
    "\n",
    "len(train_df.ix[0].values)\n",
    "train_df.values[0]\n",
    "#select pixels row, put all values an array\n",
    "X_train = train_df.filter(regex = (\"pixel.*\")).values\n",
    "X_test = test_df.filter(regex = (\"pixel.*\")).values\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "#drop to increase time for testing the distances method accuracy\n",
    "#X_test = np.delete(X_test,np.s_[100::],0)\n",
    "#y_test = np.delete(y_test, np.s_[100::])\n",
    "\n",
    "#print the sahpe\n",
    "print(X_train.shape, X_test.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training block of the kNN, nothing to do: instance based algorithm\n",
    "def train(X_train, y_train):\n",
    "    #do nthing\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define two kind of distances\n",
    "def manhattan_distance(X_train, X_test):\n",
    "    #create list for distances\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        #compute the distance\n",
    "        distance = np.sum(np.abs(X_test- X_train[i,:]))\n",
    "        #add it to list of distances\n",
    "        distances.append([distance,i])\n",
    "        \n",
    "    #sort the list\n",
    "    distances = sorted(distances)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def euclidian_distance(X_train, X_test):\n",
    "    #create list for distance\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        #compute the distance\n",
    "        distance = np.sqrt(np.sum(np.square(X_test - X_train[i,:])))\n",
    "        #add it to list of distances\n",
    "        distances.append([distance,i])\n",
    "        \n",
    "    #sort the list\n",
    "    distances = sorted(distances)\n",
    "    \n",
    "    return distances\n",
    "    \n",
    "\n",
    "#Predict block with manhattan distance\n",
    "def predict(X_train, y_train, X_test, k):\n",
    "    #create list for distances and labels\n",
    "    distances = manhattan_distance(X_train, X_test)\n",
    "    labels = []\n",
    "    #make a list of the k neighbors'targets\n",
    "    for i in range(k):\n",
    "        index = distances[i][1]\n",
    "        labels.append(y_train[index])\n",
    "    \n",
    "    #return most common label\n",
    "    return Counter(labels).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "def kNN(X_train,y_train,X_test, predictions, k):\n",
    "        #train on the input data\n",
    "        train(X_train, y_train)\n",
    "        #loop over all observations\n",
    "        for i in range(len(X_test)):\n",
    "            predictions.append(predict(X_train, y_train, X_test[i, :], k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time issues\n",
    "\n",
    "One way to cut down the curse of dimensionality of our set is to try to decompose the data and restructure it using some technics like:\n",
    "\n",
    "* KD-tree\n",
    "* Ball tree\n",
    "* Principal component analysis\n",
    "\n",
    "We choos the PCA algorithm already implementend in the sklearn library\n",
    "\n",
    "* Separate the feature space in visible cluster for 2 components\n",
    "* Try to capture the most of the variance in the dataset predicting how the prediction is good regarding the number of components\n",
    "* Choose a #of components avoiding overfitting, regarding the function\n",
    "* Compute the transform sets with the kNN algorithm increasing the speed of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "transform = pca.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 50)\n",
    "pca.fit(X_train)\n",
    "transform_train = pca.transform(X_train)\n",
    "transform_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm with the transform train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7671.174985408783\n"
     ]
    }
   ],
   "source": [
    "#Run the algorithm\n",
    "predictions = []\n",
    "\n",
    "#there divide in 10 tasks\n",
    "tic = time.time()\n",
    "kNN(transform_train, y_train, transform_test, predictions, 5)\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "\n",
    "#transform the list into an array\n",
    "predictions = np.asarray(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the output file\n",
    "out_file= open(\"predictionsK5Euc.txt\", \"w\")\n",
    "out_file.write(\"ImageID, Label\\n\")\n",
    "for i in range(len(predictions)):\n",
    "    out_file.write(str(i+1) + \",\" + str(int(predictions[i]))+ \"\\n\")\n",
    "out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
